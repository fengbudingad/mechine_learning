{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))   \n",
    "    \n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def vectorized_result(j):\n",
    "    e=np.zeros((10,1))\n",
    "    e[j]=1.0\n",
    "    return e\n",
    "\n",
    "def load(filename):\n",
    "    f=open(filename,\"r\")\n",
    "    data=json.load(f)\n",
    "    f.close()\n",
    "    cost=getattr(sys.modules[__name__],data[\"cost\"])\n",
    "    net=Network(data[\"sizes\"],cost=cost)\n",
    "    net.weights=[np.array(w) for w in data[\"weights\"]]\n",
    "    net.biases=[np.array(b) for b in data[\"biases\"]]\n",
    "    return net\n",
    "\n",
    "class CrossEntropyCost(object):\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        return (a-y)\n",
    "    \n",
    "class QuadraticCost(object):\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        return 0.5*np.linalg.norm(a-y)**2\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        return (a-y) * sigmoid_prime(z)\n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self, size,cost=CrossEntropyCost):\n",
    "        self.num_layers = len(size)\n",
    "        self.size = size\n",
    "        self.default_weight_initializer()\n",
    "        self.cost=cost\n",
    "        \n",
    "    def default_weight_initializer(self):\n",
    "        self.biases=[np.random.randn(y,1) for y in self.size[1:]]\n",
    "        self.weights=[np.random.randn(y,x)/np.square(x) for x,y in zip(self.size[:-1],self.size[1:])]\n",
    "        \n",
    "    def large_weight_initializer(self):\n",
    "        self.biases=[np.random.randn(y,1) for y in self.size[1:]]\n",
    "        self.weights=[np.random.randn(y,x) for x,y in zip(self.size[:-1],self.size[1:])]\n",
    "        \n",
    "    #前馈计算输出    \n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "    #随机梯度下降\n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,lmbda=0.0,evaluation_data=None,\n",
    "           monitor_evaluation_cost=False,\n",
    "           monitor_evaluation_accuracy=False,\n",
    "           monitor_training_cost=False,\n",
    "           monitor_training_accuracy=False):\n",
    "        if evaluation_data:\n",
    "            n_data=len(evaluation_data)\n",
    "        n=len(training_data)\n",
    "        evaluation_cost,evaluation_accuracy=[],[]\n",
    "        training_cost,training_accuracy=[],[]\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches=[training_data[k:k+mini_batch_size] for k in range(0,n,mini_batch_size)]\n",
    "            for mini_batch in mini_batches: \n",
    "                self.update_mini_batch(mini_batch,eta,lmbda,len(training_data))\n",
    "            print(\"Epoch %s training complete\"%j)\n",
    "            if monitor_training_cost:\n",
    "                cost=self.total_cost(training_data,lmbda)\n",
    "                training_cost.append(cost)\n",
    "                print(\"Cost on training data:{}\".format(cost))\n",
    "            if monitor_training_accuracy:\n",
    "                accuracy=self.accuracy(training_data,convert=True)\n",
    "                training_accuracy.append(accuracy)\n",
    "                print(\"Accuracy on training data:{}/{}\".format(accuracy,n))\n",
    "            if monitor_evaluation_cost:\n",
    "                cost=self.total_cost(evaluation_data,lmbda,convert=True)\n",
    "                evaluation_cost.append(cost)\n",
    "                print(\"Cost on evaluation data:{}\".format(cost))\n",
    "            if monitor_evaluation_accuracy:\n",
    "                accuracy=self.accuracy(evaluation_data)\n",
    "                evaluation_accuracy.append(accuracy)\n",
    "            print(\"Accuracy on evaluation data:{}/{}\".format(self.accuracy(evaluation_data),n_data))\n",
    "        return evaluation_cost,evaluation_accuracy,training_cost,training_accuracy\n",
    "    \n",
    "    #更新权值与偏置\n",
    "    def update_mini_batch(self, mini_batch, eta,lmbda,n):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
    "    \n",
    "    #反向传播求偏导\n",
    "    def backprop(self, x, y):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] \n",
    "        zs = [] \n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        \n",
    "        delta = self.cost.delta(zs[-1],activations[-1], y)\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "    \n",
    "    def accuracy(self,data,convert=False):\n",
    "        if convert:\n",
    "            results=[(np.argmax(self.feedforward(x)),np.argmax(y)) for (x,y) in data]\n",
    "        else:\n",
    "            results=[(np.argmax(self.feedforward(x)),y) for (x,y) in data]\n",
    "        return sum(int(x==y) for (x,y) in results)\n",
    "\n",
    "    def total_cost(self,data,lmbda,convert=False):\n",
    "        cost=0.0\n",
    "        for x,y in data:\n",
    "            a=self.feedforward(x)\n",
    "            if convert:\n",
    "                y=vectorized_result(y)\n",
    "            cost+=self.cost.fn(a,y)/len(data)\n",
    "        cost+=0.5*(lmbda/len(data))*sum(np.linalg.norm(w)**2 for w in self.weights)\n",
    "        return cost\n",
    "    \n",
    "    def save(self,filename):\n",
    "        data={\"size\":self.size,\"weights\":[w.tolist() for w in self.weights],\n",
    "             \"biases\":[b.tolist() for b in self.biases],\n",
    "             \"cost\":str(self.cost.__name__)}\n",
    "        f=open(filename,\"w\")\n",
    "        json.dump(data,f)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-e6f8df7915f7>:65: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"SGD\" failed type inference due to: \u001b[1m\u001b[1mnon-precise type pyobject\u001b[0m\n",
      "\u001b[0m\u001b[1m[1] During: typing of argument at <ipython-input-28-e6f8df7915f7> (71)\u001b[0m\n",
      "\u001b[1m\n",
      "File \"<ipython-input-28-e6f8df7915f7>\", line 71:\u001b[0m\n",
      "\u001b[1m    def SGD(self, training_data, epochs, mini_batch_size, eta,lmbda=0.0,evaluation_data=None,\n",
      "        <source elided>\n",
      "           monitor_training_accuracy=False):\n",
      "\u001b[1m        if evaluation_data:\n",
      "\u001b[0m        \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  @jit\n",
      "<ipython-input-28-e6f8df7915f7>:65: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"SGD\" failed type inference due to: \u001b[1m\u001b[1mcannot determine Numba type of <class 'numba.dispatcher.LiftedLoop'>\u001b[0m\n",
      "\u001b[1m\n",
      "File \"<ipython-input-28-e6f8df7915f7>\", line 76:\u001b[0m\n",
      "\u001b[1m    def SGD(self, training_data, epochs, mini_batch_size, eta,lmbda=0.0,evaluation_data=None,\n",
      "        <source elided>\n",
      "        training_cost,training_accuracy=[],[]\n",
      "\u001b[1m        for j in range(epochs):\n",
      "\u001b[0m        \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "  @jit\n",
      "E:\\Python37\\Lib\\site-packages\\numba\\object_mode_passes.py:178: NumbaWarning: \u001b[1mFunction \"SGD\" was compiled in object mode without forceobj=True, but has lifted loops.\n",
      "\u001b[1m\n",
      "File \"<ipython-input-28-e6f8df7915f7>\", line 66:\u001b[0m\n",
      "\u001b[1m    @jit\n",
      "\u001b[1m    def SGD(self, training_data, epochs, mini_batch_size, eta,lmbda=0.0,evaluation_data=None,\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  state.func_ir.loc))\n",
      "E:\\Python37\\Lib\\site-packages\\numba\\object_mode_passes.py:187: NumbaDeprecationWarning: \u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"<ipython-input-28-e6f8df7915f7>\", line 66:\u001b[0m\n",
      "\u001b[1m    @jit\n",
      "\u001b[1m    def SGD(self, training_data, epochs, mini_batch_size, eta,lmbda=0.0,evaluation_data=None,\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg, state.func_ir.loc))\n",
      "<ipython-input-28-e6f8df7915f7>:65: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"SGD\" failed type inference due to: \u001b[1m\u001b[1mnon-precise type pyobject\u001b[0m\n",
      "\u001b[0m\u001b[1m[1] During: typing of argument at <ipython-input-28-e6f8df7915f7> (76)\u001b[0m\n",
      "\u001b[1m\n",
      "File \"<ipython-input-28-e6f8df7915f7>\", line 76:\u001b[0m\n",
      "\u001b[1m    def SGD(self, training_data, epochs, mini_batch_size, eta,lmbda=0.0,evaluation_data=None,\n",
      "        <source elided>\n",
      "        training_cost,training_accuracy=[],[]\n",
      "\u001b[1m        for j in range(epochs):\n",
      "\u001b[0m        \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  @jit\n",
      "E:\\Python37\\Lib\\site-packages\\numba\\object_mode_passes.py:178: NumbaWarning: \u001b[1mFunction \"SGD\" was compiled in object mode without forceobj=True.\n",
      "\u001b[1m\n",
      "File \"<ipython-input-28-e6f8df7915f7>\", line 76:\u001b[0m\n",
      "\u001b[1m    def SGD(self, training_data, epochs, mini_batch_size, eta,lmbda=0.0,evaluation_data=None,\n",
      "        <source elided>\n",
      "        training_cost,training_accuracy=[],[]\n",
      "\u001b[1m        for j in range(epochs):\n",
      "\u001b[0m        \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  state.func_ir.loc))\n",
      "E:\\Python37\\Lib\\site-packages\\numba\\object_mode_passes.py:187: NumbaDeprecationWarning: \u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"<ipython-input-28-e6f8df7915f7>\", line 76:\u001b[0m\n",
      "\u001b[1m    def SGD(self, training_data, epochs, mini_batch_size, eta,lmbda=0.0,evaluation_data=None,\n",
      "        <source elided>\n",
      "        training_cost,training_accuracy=[],[]\n",
      "\u001b[1m        for j in range(epochs):\n",
      "\u001b[0m        \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg, state.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Cost on training data:0.5155043796797054\n",
      "Accuracy on training data:46533/50000\n",
      "Cost on evaluation data:0.6873582399226027\n",
      "Accuracy on evaluation data:9334/10000\n",
      "Epoch 1 training complete\n",
      "Cost on training data:0.4566267632364201\n",
      "Accuracy on training data:47351/50000\n",
      "Cost on evaluation data:0.7242354653833223\n",
      "Accuracy on evaluation data:9491/10000\n",
      "Epoch 2 training complete\n",
      "Cost on training data:0.3998343302033394\n",
      "Accuracy on training data:47849/50000\n",
      "Cost on evaluation data:0.7301243991180546\n",
      "Accuracy on evaluation data:9526/10000\n",
      "Epoch 3 training complete\n",
      "Cost on training data:0.37861174575337325\n",
      "Accuracy on training data:48041/50000\n",
      "Cost on evaluation data:0.7511766670512814\n",
      "Accuracy on evaluation data:9569/10000\n",
      "Epoch 4 training complete\n",
      "Cost on training data:0.3808318292089895\n",
      "Accuracy on training data:48041/50000\n",
      "Cost on evaluation data:0.7790740638224632\n",
      "Accuracy on evaluation data:9577/10000\n",
      "Epoch 5 training complete\n",
      "Cost on training data:0.3731240085226371\n",
      "Accuracy on training data:48139/50000\n",
      "Cost on evaluation data:0.796588719748411\n",
      "Accuracy on evaluation data:9577/10000\n",
      "Epoch 6 training complete\n",
      "Cost on training data:0.36197199919928197\n",
      "Accuracy on training data:48249/50000\n",
      "Cost on evaluation data:0.8118598281584495\n",
      "Accuracy on evaluation data:9579/10000\n",
      "Epoch 7 training complete\n",
      "Cost on training data:0.34662207237445686\n",
      "Accuracy on training data:48399/50000\n",
      "Cost on evaluation data:0.8028949634356523\n",
      "Accuracy on evaluation data:9612/10000\n",
      "Epoch 8 training complete\n",
      "Cost on training data:0.35623835847065133\n",
      "Accuracy on training data:48381/50000\n",
      "Cost on evaluation data:0.8186410448739486\n",
      "Accuracy on evaluation data:9622/10000\n",
      "Epoch 9 training complete\n",
      "Cost on training data:0.350249367317893\n",
      "Accuracy on training data:48423/50000\n",
      "Cost on evaluation data:0.8255695756007033\n",
      "Accuracy on evaluation data:9623/10000\n",
      "Epoch 10 training complete\n",
      "Cost on training data:0.35703728501747395\n",
      "Accuracy on training data:48416/50000\n",
      "Cost on evaluation data:0.8454994174573471\n",
      "Accuracy on evaluation data:9601/10000\n",
      "Epoch 11 training complete\n",
      "Cost on training data:0.3571884447880663\n",
      "Accuracy on training data:48379/50000\n",
      "Cost on evaluation data:0.8506034033551548\n",
      "Accuracy on evaluation data:9585/10000\n",
      "Epoch 12 training complete\n",
      "Cost on training data:0.3544320887146789\n",
      "Accuracy on training data:48460/50000\n",
      "Cost on evaluation data:0.8534158096602934\n",
      "Accuracy on evaluation data:9609/10000\n",
      "Epoch 13 training complete\n",
      "Cost on training data:0.348127284803516\n",
      "Accuracy on training data:48556/50000\n",
      "Cost on evaluation data:0.8536274573288727\n",
      "Accuracy on evaluation data:9615/10000\n",
      "Epoch 14 training complete\n",
      "Cost on training data:0.3335427308120096\n",
      "Accuracy on training data:48654/50000\n",
      "Cost on evaluation data:0.843584260102657\n",
      "Accuracy on evaluation data:9649/10000\n",
      "Epoch 15 training complete\n",
      "Cost on training data:0.33079167138212273\n",
      "Accuracy on training data:48683/50000\n",
      "Cost on evaluation data:0.8461722421967239\n",
      "Accuracy on evaluation data:9633/10000\n",
      "Epoch 16 training complete\n",
      "Cost on training data:0.33866469795833365\n",
      "Accuracy on training data:48639/50000\n",
      "Cost on evaluation data:0.8496135548080523\n",
      "Accuracy on evaluation data:9644/10000\n",
      "Epoch 17 training complete\n",
      "Cost on training data:0.3285509289791968\n",
      "Accuracy on training data:48687/50000\n",
      "Cost on evaluation data:0.8532711950648573\n",
      "Accuracy on evaluation data:9638/10000\n",
      "Epoch 18 training complete\n",
      "Cost on training data:0.3429136467440362\n",
      "Accuracy on training data:48558/50000\n",
      "Cost on evaluation data:0.865478112401948\n",
      "Accuracy on evaluation data:9606/10000\n",
      "Epoch 19 training complete\n",
      "Cost on training data:0.3264503327188591\n",
      "Accuracy on training data:48711/50000\n",
      "Cost on evaluation data:0.8480131758509442\n",
      "Accuracy on evaluation data:9653/10000\n",
      "Epoch 20 training complete\n",
      "Cost on training data:0.32734858899165475\n",
      "Accuracy on training data:48674/50000\n",
      "Cost on evaluation data:0.8546025326949147\n",
      "Accuracy on evaluation data:9647/10000\n",
      "Epoch 21 training complete\n",
      "Cost on training data:0.32655462326733414\n",
      "Accuracy on training data:48766/50000\n",
      "Cost on evaluation data:0.8466196653967679\n",
      "Accuracy on evaluation data:9659/10000\n",
      "Epoch 22 training complete\n",
      "Cost on training data:0.3284050005610196\n",
      "Accuracy on training data:48694/50000\n",
      "Cost on evaluation data:0.8547288053769835\n",
      "Accuracy on evaluation data:9650/10000\n",
      "Epoch 23 training complete\n",
      "Cost on training data:0.3334610960072634\n",
      "Accuracy on training data:48702/50000\n",
      "Cost on evaluation data:0.8714842432740684\n",
      "Accuracy on evaluation data:9628/10000\n",
      "Epoch 24 training complete\n",
      "Cost on training data:0.33440353738983203\n",
      "Accuracy on training data:48619/50000\n",
      "Cost on evaluation data:0.8689363296273603\n",
      "Accuracy on evaluation data:9626/10000\n",
      "Epoch 25 training complete\n",
      "Cost on training data:0.3266354256301823\n",
      "Accuracy on training data:48715/50000\n",
      "Cost on evaluation data:0.8598680095719096\n",
      "Accuracy on evaluation data:9630/10000\n",
      "Epoch 26 training complete\n",
      "Cost on training data:0.32082874041720333\n",
      "Accuracy on training data:48761/50000\n",
      "Cost on evaluation data:0.8610831380111155\n",
      "Accuracy on evaluation data:9651/10000\n",
      "Epoch 27 training complete\n",
      "Cost on training data:0.3203067764204158\n",
      "Accuracy on training data:48780/50000\n",
      "Cost on evaluation data:0.8552224266977068\n",
      "Accuracy on evaluation data:9662/10000\n",
      "Epoch 28 training complete\n",
      "Cost on training data:0.31840205727951076\n",
      "Accuracy on training data:48778/50000\n",
      "Cost on evaluation data:0.85931008667645\n",
      "Accuracy on evaluation data:9647/10000\n",
      "Epoch 29 training complete\n",
      "Cost on training data:0.3215373878636459\n",
      "Accuracy on training data:48762/50000\n",
      "Cost on evaluation data:0.8598718306676199\n",
      "Accuracy on evaluation data:9645/10000\n",
      "Epoch 30 training complete\n",
      "Cost on training data:0.3215218571256483\n",
      "Accuracy on training data:48805/50000\n",
      "Cost on evaluation data:0.8624871260813629\n",
      "Accuracy on evaluation data:9656/10000\n",
      "Epoch 31 training complete\n",
      "Cost on training data:0.3289510743973091\n",
      "Accuracy on training data:48695/50000\n",
      "Cost on evaluation data:0.8716448819813394\n",
      "Accuracy on evaluation data:9636/10000\n",
      "Epoch 32 training complete\n",
      "Cost on training data:0.34801574263429946\n",
      "Accuracy on training data:48586/50000\n",
      "Cost on evaluation data:0.8895975758328205\n",
      "Accuracy on evaluation data:9603/10000\n",
      "Epoch 33 training complete\n",
      "Cost on training data:0.31454127074830635\n",
      "Accuracy on training data:48853/50000\n",
      "Cost on evaluation data:0.8634132279659643\n",
      "Accuracy on evaluation data:9648/10000\n",
      "Epoch 34 training complete\n",
      "Cost on training data:0.31883960624804647\n",
      "Accuracy on training data:48848/50000\n",
      "Cost on evaluation data:0.8644774633960464\n",
      "Accuracy on evaluation data:9651/10000\n",
      "Epoch 35 training complete\n",
      "Cost on training data:0.3162536982352987\n",
      "Accuracy on training data:48865/50000\n",
      "Cost on evaluation data:0.8611361510797564\n",
      "Accuracy on evaluation data:9655/10000\n",
      "Epoch 36 training complete\n",
      "Cost on training data:0.32934678944974766\n",
      "Accuracy on training data:48732/50000\n",
      "Cost on evaluation data:0.8796777966980664\n",
      "Accuracy on evaluation data:9617/10000\n",
      "Epoch 37 training complete\n",
      "Cost on training data:0.31904328183667174\n",
      "Accuracy on training data:48810/50000\n",
      "Cost on evaluation data:0.8699277623996714\n",
      "Accuracy on evaluation data:9637/10000\n",
      "Epoch 38 training complete\n",
      "Cost on training data:0.32428643529571344\n",
      "Accuracy on training data:48783/50000\n",
      "Cost on evaluation data:0.8745753156092229\n",
      "Accuracy on evaluation data:9636/10000\n",
      "Epoch 39 training complete\n",
      "Cost on training data:0.32683932280661854\n",
      "Accuracy on training data:48756/50000\n",
      "Cost on evaluation data:0.8750294578546004\n",
      "Accuracy on evaluation data:9636/10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.6873582399226027,\n",
       "  0.7242354653833223,\n",
       "  0.7301243991180546,\n",
       "  0.7511766670512814,\n",
       "  0.7790740638224632,\n",
       "  0.796588719748411,\n",
       "  0.8118598281584495,\n",
       "  0.8028949634356523,\n",
       "  0.8186410448739486,\n",
       "  0.8255695756007033,\n",
       "  0.8454994174573471,\n",
       "  0.8506034033551548,\n",
       "  0.8534158096602934,\n",
       "  0.8536274573288727,\n",
       "  0.843584260102657,\n",
       "  0.8461722421967239,\n",
       "  0.8496135548080523,\n",
       "  0.8532711950648573,\n",
       "  0.865478112401948,\n",
       "  0.8480131758509442,\n",
       "  0.8546025326949147,\n",
       "  0.8466196653967679,\n",
       "  0.8547288053769835,\n",
       "  0.8714842432740684,\n",
       "  0.8689363296273603,\n",
       "  0.8598680095719096,\n",
       "  0.8610831380111155,\n",
       "  0.8552224266977068,\n",
       "  0.85931008667645,\n",
       "  0.8598718306676199,\n",
       "  0.8624871260813629,\n",
       "  0.8716448819813394,\n",
       "  0.8895975758328205,\n",
       "  0.8634132279659643,\n",
       "  0.8644774633960464,\n",
       "  0.8611361510797564,\n",
       "  0.8796777966980664,\n",
       "  0.8699277623996714,\n",
       "  0.8745753156092229,\n",
       "  0.8750294578546004],\n",
       " [9334,\n",
       "  9491,\n",
       "  9526,\n",
       "  9569,\n",
       "  9577,\n",
       "  9577,\n",
       "  9579,\n",
       "  9612,\n",
       "  9622,\n",
       "  9623,\n",
       "  9601,\n",
       "  9585,\n",
       "  9609,\n",
       "  9615,\n",
       "  9649,\n",
       "  9633,\n",
       "  9644,\n",
       "  9638,\n",
       "  9606,\n",
       "  9653,\n",
       "  9647,\n",
       "  9659,\n",
       "  9650,\n",
       "  9628,\n",
       "  9626,\n",
       "  9630,\n",
       "  9651,\n",
       "  9662,\n",
       "  9647,\n",
       "  9645,\n",
       "  9656,\n",
       "  9636,\n",
       "  9603,\n",
       "  9648,\n",
       "  9651,\n",
       "  9655,\n",
       "  9617,\n",
       "  9637,\n",
       "  9636,\n",
       "  9636],\n",
       " [0.5155043796797054,\n",
       "  0.4566267632364201,\n",
       "  0.3998343302033394,\n",
       "  0.37861174575337325,\n",
       "  0.3808318292089895,\n",
       "  0.3731240085226371,\n",
       "  0.36197199919928197,\n",
       "  0.34662207237445686,\n",
       "  0.35623835847065133,\n",
       "  0.350249367317893,\n",
       "  0.35703728501747395,\n",
       "  0.3571884447880663,\n",
       "  0.3544320887146789,\n",
       "  0.348127284803516,\n",
       "  0.3335427308120096,\n",
       "  0.33079167138212273,\n",
       "  0.33866469795833365,\n",
       "  0.3285509289791968,\n",
       "  0.3429136467440362,\n",
       "  0.3264503327188591,\n",
       "  0.32734858899165475,\n",
       "  0.32655462326733414,\n",
       "  0.3284050005610196,\n",
       "  0.3334610960072634,\n",
       "  0.33440353738983203,\n",
       "  0.3266354256301823,\n",
       "  0.32082874041720333,\n",
       "  0.3203067764204158,\n",
       "  0.31840205727951076,\n",
       "  0.3215373878636459,\n",
       "  0.3215218571256483,\n",
       "  0.3289510743973091,\n",
       "  0.34801574263429946,\n",
       "  0.31454127074830635,\n",
       "  0.31883960624804647,\n",
       "  0.3162536982352987,\n",
       "  0.32934678944974766,\n",
       "  0.31904328183667174,\n",
       "  0.32428643529571344,\n",
       "  0.32683932280661854],\n",
       " [46533,\n",
       "  47351,\n",
       "  47849,\n",
       "  48041,\n",
       "  48041,\n",
       "  48139,\n",
       "  48249,\n",
       "  48399,\n",
       "  48381,\n",
       "  48423,\n",
       "  48416,\n",
       "  48379,\n",
       "  48460,\n",
       "  48556,\n",
       "  48654,\n",
       "  48683,\n",
       "  48639,\n",
       "  48687,\n",
       "  48558,\n",
       "  48711,\n",
       "  48674,\n",
       "  48766,\n",
       "  48694,\n",
       "  48702,\n",
       "  48619,\n",
       "  48715,\n",
       "  48761,\n",
       "  48780,\n",
       "  48778,\n",
       "  48762,\n",
       "  48805,\n",
       "  48695,\n",
       "  48586,\n",
       "  48853,\n",
       "  48848,\n",
       "  48865,\n",
       "  48732,\n",
       "  48810,\n",
       "  48783,\n",
       "  48756])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mnist_loader\n",
    "training_data,validation_data,test_data=mnist_loader.load_data_wrapper()\n",
    "net=Network([784,30,10],cost=CrossEntropyCost)\n",
    "net.default_weight_initializer()\n",
    "net.SGD(training_data,40,20,0.5,lmbda=5.0,evaluation_data=validation_data,\n",
    "           monitor_evaluation_cost=True,\n",
    "           monitor_evaluation_accuracy=True,\n",
    "           monitor_training_cost=True,\n",
    "           monitor_training_accuracy=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
